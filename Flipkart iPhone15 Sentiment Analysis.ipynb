{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bf3fd2",
   "metadata": {},
   "source": [
    "# Flipkart iPhone 15 (128GB) — Customer Sentiment Analysis\n",
    "\n",
    "**Author:** Harsh\n",
    "\n",
    "**Overview:** This notebook scrapes customer reviews for the iPhone 15 (128GB) from Flipkart, cleans and preprocesses the text, performs sentiment analysis using TextBlob, and produces visualizations and a short report. The code is fully commented and includes a sample input/output demo.\n",
    "\n",
    "**Environment:** Local Jupyter Notebook (Anaconda / VS Code / JupyterLab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faeecdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "✅ All required packages are installed (or already present).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 0️⃣ Auto-Installer for Required Libraries (runs inside the notebook)\n",
    "# ==============================================================\n",
    "# NOTE: On a local machine this will install packages into the Python environment\n",
    "# that Jupyter is running in. If you prefer to install manually, skip this cell.\n",
    "import sys\n",
    "print('Python executable:', sys.executable)\n",
    "!{sys.executable} -m pip install --quiet --upgrade pip\n",
    "!{sys.executable} -m pip install --quiet selenium webdriver-manager beautifulsoup4 pandas numpy textblob nltk matplotlib seaborn wordcloud tqdm\n",
    "print('✅ All required packages are installed (or already present).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824a19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# 1 — Imports and initial setup\n",
    "import time, re, os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Selenium and webdriver manager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# NLP & sentiment\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Inline plots for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "print('✅ Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad820031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ punkt_tab downloaded successfully.\n",
      "✅ All required NLTK data downloaded.\n"
     ]
    }
   ],
   "source": [
    "# 2 — Download NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# ✅ Fix for NLTK 3.9+ where 'punkt_tab' is required\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"✅ punkt_tab downloaded successfully.\")\n",
    "except:\n",
    "    print(\"⚠️ punkt_tab not available in this NLTK version (safe to ignore).\")\n",
    "\n",
    "print('✅ All required NLTK data downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02005d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ setup_driver is ready (use headless=False to see the browser)\n"
     ]
    }
   ],
   "source": [
    "# 3 — Selenium setup function (for local Jupyter)\n",
    "def setup_driver(headless=True):\n",
    "    \"\"\"Initialize a Chrome WebDriver using webdriver-manager for local execution.\n",
    "    headless: set to False if you want to watch the browser open (useful for debugging).\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless=new')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    # recommended: do not run too aggressively; webdriver-manager installs a compatible chromedriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.implicitly_wait(5)\n",
    "    return driver\n",
    "\n",
    "print('✅ setup_driver is ready (use headless=False to see the browser)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e86937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 — Scraping reviews (Selenium + BeautifulSoup)\n",
    "# IMPORTANT: Set `reviews_url` to the Flipkart 'All reviews' page for iPhone 15 (128GB).\n",
    "# The example URL below may change; replace with the accurate reviews page URL if needed.\n",
    "\n",
    "reviews_url = \"https://www.flipkart.com/apple-iphone-15-128-gb/product-reviews/itmdbebdf89e1d65?pid=MOBGTAGPAQNVKZCM\"\n",
    "TARGET_REVIEWS = 350  # target >= 300 to allow for cleaning and duplicates removal\n",
    "\n",
    "\n",
    "def extract_reviews_from_page_source(html):\n",
    "    \"\"\"Parse a Flipkart reviews page HTML and extract review dicts.\n",
    "    Returns a list of {'username','rating','review_text'}\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    reviews = []\n",
    "    # Flipkart review items are often inside divs with class '_1AtVbE' — this is a heuristic\n",
    "    cards = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "    for card in cards:\n",
    "        try:\n",
    "            rating_el = card.find('div', {'class': re.compile(r'_3LWZlK')})\n",
    "            review_el = card.find('div', {'class': 't-ZTKy'})\n",
    "            user_el = card.find('p', {'class': '_2sc7ZR'})\n",
    "            if rating_el and review_el:\n",
    "                rating_text = rating_el.get_text(strip=True)\n",
    "                rating_match = re.search(r'([1-5])', rating_text)\n",
    "                rating = int(rating_match.group(1)) if rating_match else None\n",
    "                review_text = review_el.get_text(separator=' ').replace('READ MORE', '').strip()\n",
    "                username = user_el.get_text(strip=True) if user_el else 'Unknown'\n",
    "                if rating and review_text:\n",
    "                    reviews.append({'username': username, 'rating': rating, 'review_text': review_text})\n",
    "        except Exception:\n",
    "            continue\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def scrape_flipkart_reviews(reviews_url, target_count=300, headless=True):\n",
    "    \"\"\"Scrape reviews across multiple pages until target_count is reached or pagination ends.\"\"\"\n",
    "    driver = setup_driver(headless=headless)\n",
    "    try:\n",
    "        driver.get(reviews_url)\n",
    "        time.sleep(2)\n",
    "        collected = []\n",
    "        pbar = tqdm(total=target_count, desc='Collecting reviews')\n",
    "        last_len = 0\n",
    "        # Loop until we have enough reviews or cannot get more\n",
    "        while len(collected) < target_count:\n",
    "            html = driver.page_source\n",
    "            new = extract_reviews_from_page_source(html)\n",
    "            # Add unique by review_text\n",
    "            for r in new:\n",
    "                if not any(r['review_text'].strip() == ex['review_text'].strip() for ex in collected):\n",
    "                    collected.append(r)\n",
    "                    pbar.update(1)\n",
    "                    if len(collected) >= target_count:\n",
    "                        break\n",
    "            # Attempt to click 'Next' — Flipkart uses a span with text 'Next' for pagination\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.XPATH, \"//span[text()='Next']\")\n",
    "                driver.execute_script('arguments[0].click();', next_btn)\n",
    "                time.sleep(2)\n",
    "            except NoSuchElementException:\n",
    "                # If no Next, try scrolling to load more (rare for Flipkart reviews)\n",
    "                driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                time.sleep(2)\n",
    "                # check if no new reviews were found after scroll\n",
    "                if len(collected) == last_len:\n",
    "                    break\n",
    "                last_len = len(collected)\n",
    "            # Safety guard\n",
    "            if len(collected) >= target_count:\n",
    "                break\n",
    "        pbar.close()\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return collected\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# reviews = scrape_flipkart_reviews(reviews_url, TARGET_REVIEWS, headless=True)\n",
    "# print('Collected reviews:', len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1bffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 — Data cleaning & preprocessing (Pandas + NLTK)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_review_text(text):\n",
    "    \"\"\"Lowercase, remove URLs/special chars, tokenize, remove stopwords, and lemmatize.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'&amp;|&lt;|&gt;', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words and len(tok) > 1]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "\n",
    "def build_clean_dataframe(reviews_list):\n",
    "    df = pd.DataFrame(reviews_list)\n",
    "    df.drop_duplicates(subset=['review_text'], inplace=True)\n",
    "    df.dropna(subset=['rating','review_text'], inplace=True)\n",
    "    df['clean_text'] = df['review_text'].apply(clean_review_text)\n",
    "    df['review_length'] = df['clean_text'].apply(lambda x: len(str(x).split()))\n",
    "    return df\n",
    "\n",
    "# Example (if you have `reviews` list):\n",
    "# df = build_clean_dataframe(reviews)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c853c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 — Sentiment analysis using TextBlob\n",
    "\n",
    "def classify_sentiment_textblob(text, threshold=0.1):\n",
    "    tb = TextBlob(text)\n",
    "    polarity = tb.sentiment.polarity\n",
    "    subjectivity = tb.sentiment.subjectivity\n",
    "    label = 'positive' if polarity >= threshold else 'negative'\n",
    "    return polarity, subjectivity, label\n",
    "\n",
    "\n",
    "def apply_sentiment(df, text_col='clean_text', threshold=0.1):\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "    label = []\n",
    "    for t in df[text_col].astype(str):\n",
    "        p, s, l = classify_sentiment_textblob(t, threshold=threshold)\n",
    "        polarity.append(p)\n",
    "        subjectivity.append(s)\n",
    "        label.append(l)\n",
    "    df['polarity'] = polarity\n",
    "    df['subjectivity'] = subjectivity\n",
    "    df['sentiment'] = label\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = apply_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe81c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Visualization helper functions are ready\n"
     ]
    }
   ],
   "source": [
    "# 7 — Visualizations: Sentiment distribution, Rating vs Polarity, Word Clouds, Review length\n",
    "\n",
    "def plot_sentiment_distribution(df, out_path=None):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x='sentiment', data=df, order=['positive','negative'])\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.tight_layout()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rating_vs_polarity(df, out_path=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.boxplot(x='rating', y='polarity', data=df.sort_values('rating'))\n",
    "    plt.title('Polarity distribution by numeric rating')\n",
    "    plt.tight_layout()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_wordcloud(text, title=None, out_path=None):\n",
    "    if not text or len(text.strip())==0:\n",
    "        print('No text for wordcloud.')\n",
    "        return\n",
    "    wc = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_review_length_kde(df, out_path=None):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.kdeplot(df[df['sentiment']=='positive']['review_length'], label='positive')\n",
    "    sns.kdeplot(df[df['sentiment']=='negative']['review_length'], label='negative')\n",
    "    plt.xlabel('Review length (tokens)')\n",
    "    plt.title('Review length distribution by sentiment')\n",
    "    plt.legend()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path)\n",
    "    plt.show()\n",
    "\n",
    "print('✅ Visualization helper functions are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c8a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions to summarize and export results are ready\n"
     ]
    }
   ],
   "source": [
    "# 8 — Analysis summary and exporting results\n",
    "\n",
    "def summarize_and_export(df, out_dir='flipkart_output'):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    summary = {\n",
    "        'total_reviews_collected': int(len(df)),\n",
    "        'positive_count': int((df['sentiment']=='positive').sum()),\n",
    "        'negative_count': int((df['sentiment']=='negative').sum()),\n",
    "        'average_rating': float(df['rating'].mean()),\n",
    "        'average_polarity': float(df['polarity'].mean())\n",
    "    }\n",
    "    # Save CSVs and summary\n",
    "    df.to_csv(os.path.join(out_dir, 'cleaned_reviews.csv'), index=False, encoding='utf-8-sig')\n",
    "    pd.DataFrame([summary]).to_csv(os.path.join(out_dir, 'summary_stats.csv'), index=False)\n",
    "    # Plots\n",
    "    plot_sentiment_distribution(df, out_path=os.path.join(out_dir,'sentiment_distribution.png'))\n",
    "    plot_rating_vs_polarity(df, out_path=os.path.join(out_dir,'rating_vs_polarity_box.png'))\n",
    "    pos_text = ' '.join(df[df['sentiment']=='positive']['clean_text'].values)\n",
    "    neg_text = ' '.join(df[df['sentiment']=='negative']['clean_text'].values)\n",
    "    generate_wordcloud(pos_text, title='Positive Reviews Word Cloud', out_path=os.path.join(out_dir,'wordcloud_positive.png'))\n",
    "    generate_wordcloud(neg_text, title='Negative Reviews Word Cloud', out_path=os.path.join(out_dir,'wordcloud_negative.png'))\n",
    "    plot_review_length_kde(df, out_path=os.path.join(out_dir,'review_length_kde.png'))\n",
    "    print('✅ Summary & artifacts saved to', out_dir)\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "# summary = summarize_and_export(df)\n",
    "print('Functions to summarize and export results are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8041a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review: The iPhone 15 camera quality is stunning and performance is lightning fast!\n",
      "TextBlob polarity: 0.375\n",
      "Predicted sentiment (threshold 0.1): positive\n",
      "\n",
      "Demo DataFrame:\n",
      "  username  rating                                  review_text  \\\n",
      "0    Alice       5       Amazing camera and battery lasts long.   \n",
      "1      Bob       2  Device heats up and battery drains quickly.   \n",
      "2    Carol       4              Great display but a bit pricey.   \n",
      "\n",
      "                          clean_text  polarity sentiment  \n",
      "0   amazing camera battery last long  0.183333  positive  \n",
      "1  device heat battery drain quickly  0.333333  positive  \n",
      "2           great display bit pricey  0.800000  positive  \n"
     ]
    }
   ],
   "source": [
    "# 9 — Sample input/output demonstration (independent examples)\n",
    "# Sample review\n",
    "sample_review = \"The iPhone 15 camera quality is stunning and performance is lightning fast!\"\n",
    "tb = TextBlob(sample_review)\n",
    "print(\"Sample review:\", sample_review)\n",
    "print(\"TextBlob polarity:\", tb.sentiment.polarity)\n",
    "print(\"Predicted sentiment (threshold 0.1):\", \"positive\" if tb.sentiment.polarity >= 0.1 else \"negative\")\n",
    "\n",
    "# Small synthetic dataset demo to validate the pipeline without scraping\n",
    "synthetic_reviews = [\n",
    "    {'username':'Alice','rating':5,'review_text':'Amazing camera and battery lasts long.'},\n",
    "    {'username':'Bob','rating':2,'review_text':'Device heats up and battery drains quickly.'},\n",
    "    {'username':'Carol','rating':4,'review_text':'Great display but a bit pricey.'}\n",
    "]\n",
    "# Build and analyze\n",
    "df_demo = build_clean_dataframe(synthetic_reviews)\n",
    "df_demo = apply_sentiment(df_demo)\n",
    "print('\\nDemo DataFrame:')\n",
    "print(df_demo[['username','rating','review_text','clean_text','polarity','sentiment']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
